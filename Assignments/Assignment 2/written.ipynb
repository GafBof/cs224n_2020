{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) As described in the doc, $\\boldsymbol{y}$ is a one-hot vector with a 1 for the true outside word $o$, that means $y_i$ is 1 if and only if $i == o$. so the proof could be below:\n",
    "\n",
    "$\\begin{aligned} - \\sum_{w\\in Vocab}y_w\\log(\\hat{y}_w) &= - [y_1\\log(\\hat{y}_1) + \\cdots + y_o\\log(\\hat{y}_o) + \\cdots + y_w\\log(\\hat{y}_w)] \\ & = - y_o\\log(\\hat{y}_o) \\ & = -\\log(\\hat{y}_o) \\ & = -\\log \\mathrm{P}(O = o | C = c) \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) we know this deravatives: $$ \\because J = CE(y, \\hat{y}) \\ \\hat{y} = softmax(\\theta)\\ \\therefore \\frac{\\partial J}{\\partial \\theta} = (\\hat{y} - y)^T $$\n",
    "\n",
    "$y$ is a column vector in the above equation. So, we can use chain rules to solve the deravitive:\n",
    "\n",
    "$$\\begin{aligned} \\frac{\\partial J}{\\partial v_c} &= \\frac{\\partial J}{\\partial \\theta} \\frac{\\partial \\theta}{\\partial v_c} \\ &= (\\hat{y} - y) \\frac{\\partial U^Tv_c}{\\partial v_c} \\ &= U^T(\\hat{y} - y)^T \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitaicondabbf1184d5db54703959bd9937dcc7060",
   "display_name": "Python 3.6.10 64-bit ('ai': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}